{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Important run notebook #003\n",
    "\n",
    "this is needed to populate the database with the stock data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/10 15:30:32 WARN Utils: Your hostname, mbp.local resolves to a loopback address: 127.0.0.1; using 192.168.18.6 instead (on interface en0)\n",
      "22/11/10 15:30:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/tmillar/dev/repo/hacking-spark-with-mongo/venv/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/tmillar/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/tmillar/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8a360749-ef3a-4371-971e-954ee3b8ff53;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 1665ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8a360749-ef3a-4371-971e-954ee3b8ff53\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/10 15:30:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/10 15:30:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/11/10 15:30:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/11/10 15:30:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/11/10 15:30:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/11/10 15:30:35 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/11/10 15:30:35 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/11/10 15:30:35 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "database = \"hacking\"\n",
    "collection = \"stocks\"\n",
    "connectionString = \"mongodb://127.0.0.1\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dates\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.0.5') \\\n",
    "    .config(\"spark.mongodb.input.uri\", connectionString) \\\n",
    "    .config(\"spark.mongodb.output.uri\", connectionString) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"database\", database) \\\n",
    "    .option(\"collection\", collection) \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-------------------+------------------+------------------+------------------+---------+--------------------+\n",
      "|         Adj Close|             Close|               Date|              High|               Low|              Open|   Volume|                 _id|\n",
      "+------------------+------------------+-------------------+------------------+------------------+------------------+---------+--------------------+\n",
      "|         27.727039|        214.009998|2010-01-04 00:00:00|        214.499996|212.38000099999996|        213.429998|123432400|636d5432962d673da...|\n",
      "|27.774976000000002|        214.379993|2010-01-05 00:00:00|        215.589994|        213.249994|        214.599998|150476200|636d5432962d673da...|\n",
      "|27.333178000000004|        210.969995|2010-01-06 00:00:00|            215.23|        210.750004|        214.379993|138040000|636d5432962d673da...|\n",
      "|          27.28265|            210.58|2010-01-07 00:00:00|        212.000006|        209.050005|            211.75|119282800|636d5432962d673da...|\n",
      "|         27.464034|211.98000499999998|2010-01-08 00:00:00|        212.000006|209.06000500000002|        210.299994|111902700|636d5432962d673da...|\n",
      "|         27.221758|210.11000299999998|2010-01-11 00:00:00|        213.000002|        208.450005|212.79999700000002|115557400|636d5432962d673da...|\n",
      "|          26.91211|        207.720001|2010-01-12 00:00:00|209.76999500000002|        206.419998|209.18999499999998|148614900|636d5432962d673da...|\n",
      "|          27.29172|        210.650002|2010-01-13 00:00:00|210.92999500000002|        204.099998|        207.870005|151473000|636d5432962d673da...|\n",
      "|         27.133657|            209.43|2010-01-14 00:00:00|210.45999700000002|        209.020004|210.11000299999998|108223500|636d5432962d673da...|\n",
      "|26.680197999999997|            205.93|2010-01-15 00:00:00|211.59999700000003|        205.869999|210.92999500000002|148516900|636d5432962d673da...|\n",
      "|27.860484999999997|        215.039995|2010-01-19 00:00:00|215.18999900000003|        207.240004|        208.330002|182501900|636d5432962d673da...|\n",
      "|         27.431644|            211.73|2010-01-20 00:00:00|        215.549994|        209.500002|        214.910006|153038200|636d5432962d673da...|\n",
      "|         26.957455|        208.069996|2010-01-21 00:00:00|213.30999599999998|        207.210003|        212.079994|152038600|636d5432962d673da...|\n",
      "|         25.620401|            197.75|2010-01-22 00:00:00|        207.499996|            197.16|206.78000600000001|220441900|636d5432962d673da...|\n",
      "|26.309658000000002|        203.070002|2010-01-25 00:00:00|        204.699999|        200.190002|202.51000200000001|266424900|636d5432962d673da...|\n",
      "|         26.681494|        205.940001|2010-01-26 00:00:00|        213.710005|        202.580004|205.95000100000001|466777500|636d5432962d673da...|\n",
      "|26.932840000000002|        207.880005|2010-01-27 00:00:00|            210.58|        199.530001|        206.849995|430642100|636d5432962d673da...|\n",
      "|25.819922000000002|        199.289995|2010-01-28 00:00:00|        205.500004|        198.699995|        204.930004|293375600|636d5432962d673da...|\n",
      "|         24.883208|        192.060003|2010-01-29 00:00:00|        202.199995|        190.250002|        201.079996|311488100|636d5432962d673da...|\n",
      "|         25.229131|        194.729998|2010-02-01 00:00:00|             196.0|191.29999899999999|192.36999699999998|187469100|636d5432962d673da...|\n",
      "+------------------+------------------+-------------------+------------------+------------------+------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- _id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform some functions on Date"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "|               6|\n",
      "|               7|\n",
      "|               8|\n",
      "|              11|\n",
      "|              12|\n",
      "|              13|\n",
      "|              14|\n",
      "|              15|\n",
      "|              19|\n",
      "|              20|\n",
      "|              21|\n",
      "|              22|\n",
      "|              25|\n",
      "|              26|\n",
      "|              27|\n",
      "|              28|\n",
      "|              29|\n",
      "|               1|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    dayofmonth,\n",
    "    dayofyear,\n",
    "    hour,\n",
    "    month,\n",
    "    year,\n",
    "    weekofyear,\n",
    "    format_number,\n",
    "    date_format\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.select(\n",
    "    dayofmonth(df[\"Date\"])\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dayofyear(Date)|\n",
      "+---------------+\n",
      "|              4|\n",
      "|              5|\n",
      "|              6|\n",
      "|              7|\n",
      "|              8|\n",
      "|             11|\n",
      "|             12|\n",
      "|             13|\n",
      "|             14|\n",
      "|             15|\n",
      "|             19|\n",
      "|             20|\n",
      "|             21|\n",
      "|             22|\n",
      "|             25|\n",
      "|             26|\n",
      "|             27|\n",
      "|             28|\n",
      "|             29|\n",
      "|             32|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    dayofyear(df[\"Date\"])\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.select(\n",
    "    hour(df[\"Date\"])\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       Date|\n",
      "+-----------+\n",
      "| Jan,4 2010|\n",
      "| Jan,5 2010|\n",
      "| Jan,6 2010|\n",
      "| Jan,7 2010|\n",
      "| Jan,8 2010|\n",
      "|Jan,11 2010|\n",
      "|Jan,12 2010|\n",
      "|Jan,13 2010|\n",
      "|Jan,14 2010|\n",
      "|Jan,15 2010|\n",
      "|Jan,19 2010|\n",
      "|Jan,20 2010|\n",
      "|Jan,21 2010|\n",
      "|Jan,22 2010|\n",
      "|Jan,25 2010|\n",
      "|Jan,26 2010|\n",
      "|Jan,27 2010|\n",
      "|Jan,28 2010|\n",
      "|Jan,29 2010|\n",
      "| Feb,1 2010|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    date_format(df[\"Date\"], \"MMM,d yyyy\").alias(\"Date\")\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Group by month then AVG Close"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_month = df.select(\n",
    "    month(df[\"Date\"]).alias(\"month\"),\n",
    "    \"Date\",\n",
    "    \"Close\"\n",
    ")\n",
    "\n",
    "df_month.groupBy(\"month\").avg().select(\"month\", \"avg(Close)\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Group by Year then Max High"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|         max(High)|\n",
      "+----+------------------+\n",
      "|2010|            326.66|\n",
      "|2011|426.69999299999995|\n",
      "|2012|        705.070023|\n",
      "|2013|        575.139999|\n",
      "|2014|        651.259979|\n",
      "|2015|134.53999299999998|\n",
      "|2016|        118.690002|\n",
      "+----+------------------+\n",
      "\n",
      "+----+------------------+\n",
      "|Year|          Max High|\n",
      "+----+------------------+\n",
      "|2012|        705.070023|\n",
      "|2014|        651.259979|\n",
      "|2013|        575.139999|\n",
      "|2011|426.69999299999995|\n",
      "|2010|            326.66|\n",
      "|2015|134.53999299999998|\n",
      "|2016|        118.690002|\n",
      "+----+------------------+\n",
      "\n",
      "+----+--------+\n",
      "|Year|Max High|\n",
      "+----+--------+\n",
      "|2012| 705.070|\n",
      "|2014| 651.260|\n",
      "|2013| 575.140|\n",
      "|2011| 426.700|\n",
      "|2010| 326.660|\n",
      "|2015| 134.540|\n",
      "|2016| 118.690|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_year = df.select(\n",
    "    year(df[\"Date\"]).alias(\"Year\"),\n",
    "    \"Date\",\n",
    "    \"High\"\n",
    ")\n",
    "\n",
    "max_df = df_year.groupBy(\"Year\").max().select(\"Year\", \"max(High)\")\n",
    "\n",
    "max_df.show()\n",
    "\n",
    "sorted_df = max_df.orderBy(max_df[\"max(High)\"].desc()).select(\"Year\", max_df[\"max(High)\"].alias(\"Max High\"))\n",
    "\n",
    "sorted_df.show()\n",
    "\n",
    "formatted_df = sorted_df.select(\"Year\", format_number(sorted_df[\"Max High\"],3).alias(\"Max High\"))\n",
    "\n",
    "formatted_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read stocks CSV and infer schema then save to MongoDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv(\"./../sample_data/stock.csv\",header=True,inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11 00:00:00|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12 00:00:00|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13 00:00:00|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14 00:00:00|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15 00:00:00|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19 00:00:00|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20 00:00:00|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21 00:00:00|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22 00:00:00|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25 00:00:00|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26 00:00:00|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27 00:00:00|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28 00:00:00|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01 00:00:00|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()\n",
    "df_csv.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}